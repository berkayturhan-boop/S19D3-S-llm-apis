{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini ve LangChain ile LLM API'larÄ±nÄ± Ã‡aÄŸÄ±rma GiriÅŸ ğŸ¦œğŸ”—\n",
    "\n",
    "Bu notebook'ta LangChain aracÄ±lÄ±ÄŸÄ±yla LLM API'larÄ±nÄ± nasÄ±l kullanacaÄŸÄ±nÄ±zÄ± Ã¶ÄŸreneceksiniz. Ã–rnek olarak Google'Ä±n Gemini API'sÄ±nÄ± kullanacaÄŸÄ±z. Bu notebook'un sonunda, LangChain kullanarak API Ã§aÄŸrÄ±larÄ± yapmayÄ± ve bunu neden yaptÄ±ÄŸÄ±mÄ±zÄ± bileceksiniz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Kurulum\n",
    "\n",
    "ğŸ‘‰ Kurulum aÅŸamasÄ±nda oluÅŸturduÄŸumuz `.env` dosyasÄ±ndaki ortam deÄŸiÅŸkenlerini yÃ¼klemek iÃ§in aÅŸaÄŸÄ±daki hÃ¼creyi Ã§alÄ±ÅŸtÄ±rÄ±n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() # Load environment variables from .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘‰ HÃ¼crenin Ã§Ä±ktÄ±sÄ± \"`True`\" mu? Harika! ArtÄ±k Gemini API ile kimlik doÄŸrulamasÄ± yapmak iÃ§in kullanÄ±lacak bir `GOOGLE_API_KEY` ortam deÄŸiÅŸkeni kurmuÅŸ olduk.\n",
    "\n",
    "EÄŸer deÄŸilse, yardÄ±m isteyin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basit Bir API Ã‡aÄŸrÄ±sÄ± Yapma\n",
    "\n",
    "Bu notebook'ta ÅŸunlarÄ±n nasÄ±l yapÄ±lacaÄŸÄ±nÄ± gÃ¶stereceÄŸiz:\n",
    "1. Google'Ä±n kendi kÃ¼tÃ¼phanesini kullanarak API Ã§aÄŸrÄ±sÄ± yapma.\n",
    "2. AynÄ± iÅŸlemi LangChain kullanarak yapma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Generative AI KÃ¼tÃ¼phanesini Kullanma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    contents=\"What is the capital of France?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`response` nesnesine bir gÃ¶z atalÄ±m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is **Paris**.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.candidates[0].content.parts[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GerÃ§ek cevabÄ± nasÄ±l alabileceÄŸinizi gÃ¶rÃ¼yor musunuz?\n",
    "\n",
    "Neyse ki, cevabÄ± hemen almak iÃ§in sadece `.text` Ã¶zelliÄŸini kullanabiliriz. Deneyin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is **Paris**.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini cevaplarÄ±nÄ± Markdown formatÄ±nda dÃ¶ndÃ¼rÃ¼r. Bunu kullanalÄ±m!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The capital of France is **Paris**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OluÅŸturma parametrelerini de deÄŸiÅŸtirebilirsiniz. `google.genai` kullanarak bunu ÅŸu ÅŸekilde yaparsÄ±nÄ±z:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types # We need to import types for the config\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    contents=\"Write a social media post about how much you're learning about transformers.\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        max_output_tokens=200,\n",
    "        temperature=1.0\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are a few options for a social media post about learning about transformers, with different tones and focuses:\n",
       "\n",
       "**Option 1: Enthusiastic & Concise**\n",
       "\n",
       "> Wow, diving deep into the world of Transformers lately and my mind is officially blown! ğŸ¤¯ The elegance of the attention mechanism and how it revolutionizes NLP is just incredible. So much to learn, so little time (but I'm soaking it all up!). #Transformers #DeepLearning #NLP #AI #MachineLearning\n",
       "\n",
       "**Option 2: Slightly More Technical & Reflective**\n",
       "\n",
       "> Currently on a journey to understand Transformers from the ground up. From the self-attention mechanism to positional encoding and beyond, it's truly a paradigm shift in how we approach sequential data. Feeling like I'm truly grasping the underlying principles and it's super rewarding! ğŸ’ª #TransformerModels #NeuralNetworks #AIResearch #DataScience\n",
       "\n",
       "**Option 3: Humorous & Relatable**\n",
       "\n",
       "> My brain is currently"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Harika. Ancak baÅŸka bir API denemek istediÄŸinizi dÃ¼ÅŸÃ¼nÃ¼n, Ã¶rneÄŸin OpenAI'nin veya Anthropic'in?\n",
    "\n",
    "OnlarÄ±n dokÃ¼mantasyonlarÄ±nÄ± incelemek ve tÃ¼m kodunuzu onlarÄ±n API'sini kullanacak ÅŸekilde yeniden yazmak zorunda kalÄ±rsÄ±nÄ±z. Tabii ki benzer olacaktÄ±r, ancak aynÄ± olmayacaktÄ±r.\n",
    "\n",
    "Neyse ki LangChain var!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Kullanma ğŸ¦œğŸ”—"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neden LangChain kullanÄ±rsÄ±nÄ±z?\n",
    "\n",
    "1. **Model-BaÄŸÄ±msÄ±z Kod**\n",
    "\n",
    "   LangChain, farklÄ± LLM saÄŸlayÄ±cÄ±larÄ± (Google, OpenAI, Anthropic, vb.) arasÄ±nda minimal kod deÄŸiÅŸikliÄŸi ile geÃ§iÅŸ yapmanÄ±zÄ± saÄŸlayan soyutlamalar sunar. Google API'sine doÄŸrudan kod yazarsanÄ±z, saÄŸlayÄ±cÄ± deÄŸiÅŸtirmek Ã¶nemli Ã¶lÃ§Ã¼de yeniden dÃ¼zenleme gerektirir.\n",
    "\n",
    "2. **BirleÅŸik ArayÃ¼z**\n",
    "\n",
    "   LangChain, altta yatan API'den baÄŸÄ±msÄ±z olarak farklÄ± LLM saÄŸlayÄ±cÄ±larÄ± arasÄ±nda etkileÅŸimleri standartlaÅŸtÄ±rÄ±r ve tutarlÄ± yÃ¶ntemler ile yanÄ±t formatlarÄ± sunar.\n",
    "\n",
    "3. **BileÅŸenlerle Ã‡alÄ±ÅŸabilirlik**\n",
    "\n",
    "   LangChain'in zincir ve pipeline mimarisi, tÃ¼m alt yapÄ±yÄ± kendiniz halletmeden prompt, bellek ve eriÅŸim sistemlerini birleÅŸtiren karmaÅŸÄ±k iÅŸ akÄ±ÅŸlarÄ± oluÅŸturmayÄ± kolaylaÅŸtÄ±rÄ±r.\n",
    "\n",
    "4. **YerleÅŸik AraÃ§lar**\n",
    "\n",
    "   LangChain, Ã§Ä±ktÄ± ayrÄ±ÅŸtÄ±rma, prompt ÅŸablonlarÄ± ve kendiniz uygulamanÄ±z gereken diÄŸer yardÄ±mcÄ± araÃ§larÄ± iÃ§erir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LangChain'in chat entegrasyonlarÄ± listesi](https://docs.langchain.com/oss/python/integrations/chat)'ne gidin ve entegrasyon listesine bakÄ±n. Favori LLM saÄŸlayÄ±cÄ±nÄ±zÄ± bulabiliyor musunuz?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kodumuzda `chat_models.ChatGoogleGenerativeAI` kullanmak istemiyoruz Ã§Ã¼nkÃ¼ bu Ã¶zellikle Gemini iÃ§in yapÄ±lmÄ±ÅŸ. LLM'yi deÄŸiÅŸtirmek istersek, modeli baÅŸlatma ÅŸeklimizi deÄŸiÅŸtirmek zorunda kalÄ±rÄ±z. Neyse ki LangChain bir modeli baÅŸlatmak iÃ§in daha genel bir yol sunar.\n",
    "\n",
    "Gemini'yi tekrar kullanalÄ±m, ancak ÅŸimdi LangChain'in genel Chat Models'ini kullanarak.\n",
    "\n",
    "ğŸ‘‰ [LangChain'in \"Models\" dokÃ¼mantasyonu](https://docs.langchain.com/oss/python/langchain/models) sayfasÄ±na gidin ve Gemini kullanarak bir chat modelinin nasÄ±l baÅŸlatÄ±lacaÄŸÄ±nÄ± bulun.\n",
    "\n",
    "Ä°puÃ§larÄ±:\n",
    "1. Hemen \"Basic Usage\" bÃ¶lÃ¼mÃ¼ne gidin.\n",
    "2. Kullanmak istediÄŸiniz modeli seÃ§erek doÄŸru dokÃ¼mantasyonu hemen gÃ¶rebilirsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Garanti Ã–nlemi: Anahtar ismini LangChain'in sevdiÄŸi hale getiriyoruz\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = os.environ.get(\"GEMINI_API_KEY\")\n",
    "\n",
    "# Modeli \"generic\" (genel) bir ÅŸekilde baÅŸlatÄ±yoruz.\n",
    "# model_provider=\"google_genai\" diyerek kime baÄŸlanacaÄŸÄ±nÄ± sÃ¶ylÃ¼yoruz.\n",
    "model = init_chat_model(\"gemini-1.5-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelin en temel kullanÄ±mÄ± sadece `.invoke()` metodunu kullanmaktÄ±r:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# Modele sorumuzu soruyoruz (Invoke ediyoruz)\n",
    "response = model.invoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YanÄ±ta bir gÃ¶z atalÄ±m. Nesnenin tÃ¼m Ã¶znitelik ve metodlarÄ±nÄ± iÃ§eren `__dict__`'ini gÃ¼zel ÅŸekilde yazdÄ±rmak iÃ§in `pprint()` kullanÄ±yoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_kwargs': {},\n",
      " 'content': 'The capital of France is **Paris**.',\n",
      " 'id': 'lc_run--019c067f-a8c8-70e1-b437-804dbaf21bf0-0',\n",
      " 'invalid_tool_calls': [],\n",
      " 'name': None,\n",
      " 'response_metadata': {'finish_reason': 'STOP',\n",
      "                       'model_name': 'gemini-2.5-flash-lite',\n",
      "                       'model_provider': 'google_genai',\n",
      "                       'safety_ratings': []},\n",
      " 'tool_calls': [],\n",
      " 'type': 'ai',\n",
      " 'usage_metadata': {'input_token_details': {'cache_read': 0},\n",
      "                    'input_tokens': 8,\n",
      "                    'output_tokens': 8,\n",
      "                    'total_tokens': 16}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(response.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CevabÄ± Ã§Ä±karÄ±n ve gÃ¶rÃ¼ntÃ¼leyin. Markdown formatÄ±nda olduÄŸunu unutmayÄ±n, bu yÃ¼zden gÃ¼zel gÃ¶rÃ¼nmesini saÄŸlayabilirsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The capital of France is **Paris**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# CevabÄ±n sadece metin kÄ±smÄ±nÄ± alÄ±p Markdown olarak basÄ±yoruz\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelin temperature deÄŸerini `.temperature` Ã¶zniteliÄŸine eriÅŸerek kontrol edebilirsiniz. Deneyin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeli kullanmadan Ã¶nce, Ã¶zniteliklere yeni deÄŸerler atayarak oluÅŸturma parametrelerini de ayarlayabiliriz.\n",
    "\n",
    "Daha Ã¶nce Google'Ä±n kÃ¼tÃ¼phanesini kullanarak sosyal medya gÃ¶nderisi yazmak iÃ§in yaptÄ±ÄŸÄ±mÄ±zÄ±n eÅŸdeÄŸerini kodlamaya Ã§alÄ±ÅŸÄ±n.\n",
    "\n",
    "> _Not_: Normal olarak modelin `max_output_tokens` deÄŸerini ayarlayabilmemiz gerekir (modeli baÅŸlatÄ±rken veya daha sonra Ã¶zniteliÄŸi deÄŸiÅŸtirerek). _langchain_google_genai_'nin mevcut sÃ¼rÃ¼mÃ¼ (4.1.1) bir [hataya](https://github.com/langchain-ai/langchain-google/issues/1454) sahip ve bu Ã§alÄ±ÅŸmÄ±yor. GeÃ§ici Ã§Ã¶zÃ¼m? `max_output_tokens`'Ä± `.invoke()` metodunun bir parametresi olarak ayarlayÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "steps"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are a few options for a social media post about learning about transformers, ranging in tone and focus. Choose the one that best fits your style!\n",
       "\n",
       "---\n",
       "\n",
       "**Option 1: Enthusiastic & Slightly Technical**\n",
       "\n",
       "ğŸ¤¯ My brain is buzzing with new knowledge! I've been diving deep into the world of **Transformers** lately, and wow, the architectural elegance is just mind-blowing. The attention mechanism, the self-attention magic... it's truly revolutionizing NLP. So much to absorb, but I'm loving every minute of this learning journey! #Transformers #MachineLearning #DeepLearning #NLP #AI #TechLearning #NeverStopLearning\n",
       "\n",
       "---\n",
       "\n",
       "**Option 2: Relatable & Fun**\n",
       "\n",
       "Okay, I'll admit it: I'm officially obsessed with **Transformers**! ğŸ¤– My to-read list is now 90% research papers and tutorials. It's like unlocking a whole new level of understanding for how AI processes"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# 1. Set the maximum number of output tokens to 200\n",
    "# (Not: Bug olduÄŸu iÃ§in bu ayarÄ± aÅŸaÄŸÄ±da invoke fonksiyonunun iÃ§ine gÃ¶meceÄŸiz)\n",
    "\n",
    "# 2. Set the temperature to 1.0\n",
    "# Temperature 1.0 = Daha yaratÄ±cÄ±, daha risk alan, daha Ã§Ä±lgÄ±n cevaplar!\n",
    "model.temperature = 1.0\n",
    "\n",
    "# 3. Generate a response with the new settings\n",
    "# \"Transformers Ã¶ÄŸreniyorum\" konulu bir sosyal medya postu istiyoruz.\n",
    "# max_output_tokens=200 parametresini BURAYA ekliyoruz (GeÃ§ici Ã§Ã¶zÃ¼m).\n",
    "response = model.invoke(\n",
    "    \"Write a social media post about how much you're learning about transformers.\",\n",
    "    max_output_tokens=200\n",
    ")\n",
    "\n",
    "# 4. Display the response\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bunun avantajÄ±? Bu LangChain Chat Model birÃ§ok baÅŸka API'yi destekleyebilir.\n",
    "\n",
    "BaÅŸka bir modele geÃ§mek iÃ§in deÄŸiÅŸtirmeniz gereken tek ÅŸeyler:\n",
    "1. DiÄŸer model iÃ§in bir API anahtarÄ± alÄ±n ve kodunuzda tanÄ±mlayÄ±n.\n",
    "2. Modeli baÅŸlatÄ±rken model ve saÄŸlayÄ±cÄ±yÄ± deÄŸiÅŸtirin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ã‡oklu Mesajlar\n",
    "\n",
    "`.invoke()` fonksiyonunu sadece tek bir mesajla kullanmak biraz kÄ±sÄ±tlayÄ±cÄ±.\n",
    "\n",
    "Åu gibi birden fazla mesaj saÄŸlayabilirsiniz:\n",
    "- `SystemMessage` veya sistem mesajlarÄ±: modelin nasÄ±l davranacaÄŸÄ±nÄ± sÃ¶ylemek iÃ§in\n",
    "- `HumanMessage` veya KullanÄ±cÄ± mesajlarÄ±: kullanÄ±cÄ±dan gelen girdi\n",
    "- `AIMessage` veya Asistan mesajlarÄ±: modelden gelen yanÄ±t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bir sosyal medya yazarÄ± yapalÄ±m.\n",
    "\n",
    "Modele nasÄ±l davranacaÄŸÄ±nÄ± aÃ§Ä±klayan bir sistem mesajÄ± gÃ¶ndereceÄŸiz. Sonra kullanÄ±cÄ± mesajÄ±nda, kendimizi sadece yazacaÄŸÄ± konuyu vermekle sÄ±nÄ±rlayabiliriz.\n",
    "\n",
    "Bunu nasÄ±l yapacaÄŸÄ±nÄ±zÄ± Ã¶ÄŸrenmek iÃ§in [LangChain'in \"Messages\" dokÃ¼mantasyonu](https://docs.langchain.com/oss/python/langchain/messages)'na bakÄ±n.\n",
    "\n",
    "Sistem mesajÄ± iÃ§in ilhama mÄ± ihtiyacÄ±nÄ±z var? Ä°ÅŸte baÅŸlamanÄ±z iÃ§in temel bir talimat:\n",
    "\n",
    "```python\n",
    "\"\"\"Sen Ãœretken AI Ã¶ÄŸrencisi iÃ§in gÃ¶nderiler yazan yaratÄ±cÄ± bir sosyal medya yazarÄ±sÄ±n.\n",
    "GÃ¶nderilerinde her zaman kelime oyunu ve harekete geÃ§irici Ã§aÄŸrÄ± bulunur.\n",
    "GÃ¶nderilerin maksimum 200 karakter uzunluÄŸundadÄ±r.\n",
    "Her zaman emoji kullanÄ±rsÄ±n.\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "steps"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Transformers, metin Ã§evirisini dÃ¶nÃ¼ÅŸtÃ¼rdÃ¼! ğŸ¤– Bu mimari, bÃ¼yÃ¼k dil modellerinin temel taÅŸÄ±dÄ±r ve yapay zeka dÃ¼nyasÄ±nda devrim yarattÄ±. GeleceÄŸe hazÄ±rsanÄ±z, Ã¶ÄŸrenmeye baÅŸlayÄ±n! ğŸ’ª #YapayZeka #DerinÃ–ÄŸrenme #NLP"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Gerekli sÄ±nÄ±flarÄ± iÃ§eri alÄ±yoruz\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from IPython.display import Markdown\n",
    "\n",
    "# 2. Mesaj listesini oluÅŸturuyoruz (Rol daÄŸÄ±lÄ±mÄ± ve Senaryo)\n",
    "messages = [\n",
    "    SystemMessage(content=\"Sen Ãœretken AI Ã¶ÄŸrencisi iÃ§in gÃ¶nderiler yazan yaratÄ±cÄ± bir sosyal medya yazarÄ±sÄ±n. GÃ¶nderilerinde her zaman kelime oyunu ve harekete geÃ§irici Ã§aÄŸrÄ± bulunur. GÃ¶nderilerin maksimum 200 karakter uzunluÄŸundadÄ±r. Her zaman emoji kullanÄ±rsÄ±n.\"),\n",
    "    HumanMessage(content=\"Transformers mimarisi hakkÄ±nda bir tweet at.\")\n",
    "]\n",
    "\n",
    "# 3. Modeli bu senaryoyla Ã§alÄ±ÅŸtÄ±rÄ±yoruz\n",
    "response = model.invoke(messages)\n",
    "\n",
    "# 4. CevabÄ± ekrana basÄ±yoruz\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ Tebrikler! ArtÄ±k LangChain kullanarak Ã§oklu mesajlarla temel prompt yazma konusunda uzmanlaÅŸtÄ±nÄ±z."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workintech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
